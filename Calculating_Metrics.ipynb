{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calculating Metrics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxAAUHPWIJxuLzrTPQ29X+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shifath472533/Evaluation-Metrics-for-Image-captioning-in-python-3/blob/master/Calculating_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5JdPbUiO1s",
        "colab_type": "text"
      },
      "source": [
        "## Using nlgeval Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW0ic3DViDfd",
        "colab_type": "code",
        "outputId": "eb5fabef-d56c-4a6c-c338-f3bb7d1e4e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "!pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Maluuba/nlg-eval.git@master\n",
            "  Cloning https://github.com/Maluuba/nlg-eval.git (to revision master) to /tmp/pip-req-build-8ufpv8n7\n",
            "  Running command git clone -q https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-8ufpv8n7\n",
            "Requirement already satisfied: click>=6.3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (7.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.17.5)\n",
            "Collecting psutil>=5.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (2.21.0)\n",
            "Requirement already satisfied: six>=1.11 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (0.22.1)\n",
            "Requirement already satisfied: gensim>=3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.6.0)\n",
            "Requirement already satisfied: Theano>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.0.4)\n",
            "Requirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (4.28.1)\n",
            "Collecting xdg\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/7b/6ad85311fd715df37ef9bb17ad1b26e26b4cdd69c7e1e7e285422b83a7e1/xdg-4.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->nlg-eval==2.3) (0.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3->nlg-eval==2.3) (1.9.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.15.2)\n",
            "Building wheels for collected packages: nlg-eval, psutil\n",
            "  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlg-eval: filename=nlg_eval-2.3-cp36-none-any.whl size=68175138 sha256=cf527a19cbe59a2803114436b976d4d390d9ab65faa26b9de964291a1b9193ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0qb89m7n/wheels/a5/7c/fd/f312beca2adcc3f49cb40570730658dad37bb5709f5d237a56\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=272662 sha256=424d67ff5de109387969a2714203fad86f3919e018218fb53204070226e82981\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "Successfully built nlg-eval psutil\n",
            "Installing collected packages: psutil, xdg, nlg-eval\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed nlg-eval-2.3 psutil-5.7.0 xdg-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0GjJ3P_i0Nu",
        "colab_type": "code",
        "outputId": "f3f65b9f-881a-41c7-c257-010c2ff53b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "!nlg-eval --setup"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "\u001b[31mInstalling to /root/.cache/nlgeval\u001b[0m\n",
            "\u001b[31mIn case of incomplete downloads, delete the directory and run `nlg-eval --setup /root/.cache/nlgeval' again.\u001b[0m\n",
            "Downloading http://nlp.stanford.edu/data/glove.6B.zip to /root/.cache/nlgeval.\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/dictionary.txt to /root/.cache/nlgeval.\n",
            "Downloading https://raw.githubusercontent.com/robmsmt/glove-gensim/4c2224bccd61627b76c50a5e1d6afd1c82699d22/glove2word2vec.py to /usr/local/lib/python3.6/dist-packages/nlgeval/word2vec.\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/utable.npy to /root/.cache/nlgeval.\n",
            "glove2word2vec.py: 100% 1.00/1.00 [00:00<00:00, 443 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/btable.npy to /root/.cache/nlgeval.\n",
            "dictionary.txt: 550 chunks [00:00, 824 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz to /root/.cache/nlgeval.\n",
            "uni_skip.npz: 100% 634/634 [00:19<00:00, 33.3 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl to /root/.cache/nlgeval.\n",
            "uni_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.02k chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz to /root/.cache/nlgeval.\n",
            "bi_skip.npz: 100% 276/276 [00:10<00:00, 27.0 chunks/s]\n",
            "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl to /root/.cache/nlgeval.\n",
            "bi_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.17k chunks/s]\n",
            "Downloading https://raw.githubusercontent.com/moses-smt/mosesdecoder/b199e654df2a26ea58f234cbb642e89d9c1f269d/scripts/generic/multi-bleu.perl to /usr/local/lib/python3.6/dist-packages/nlgeval/multibleu.\n",
            "multi-bleu.perl: 100% 1.00/1.00 [00:00<00:00, 261 chunks/s]\n",
            "utable.npy: 100% 2.23k/2.23k [01:16<00:00, 29.3 chunks/s]\n",
            "btable.npy: 100% 2.23k/2.23k [01:18<00:00, 28.3 chunks/s]\n",
            "glove.6B.zip: 100% 823/823 [06:26<00:00, 2.13 chunks/s]\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-02-22 06:24:43,173 : MainThread : INFO : 400000 lines with 300 dimensions\n",
            "2020-02-22 06:24:48,459 : MainThread : INFO : Model /root/.cache/nlgeval/glove.6B.300d.model.txt successfully created !!\n",
            "2020-02-22 06:24:48,459 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-02-22 06:26:47,358 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-02-22 06:26:47,358 : MainThread : INFO : precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "2020-02-22 06:26:50,277 : MainThread : INFO : Most similar to king are: [('queen', 0.6336469054222107), ('prince', 0.619662344455719), ('monarch', 0.5899620652198792), ('kingdom', 0.5791267156600952), ('throne', 0.5606487989425659), ('ii', 0.5562329888343811), ('iii', 0.5503199100494385), ('crown', 0.5224862694740295), ('reign', 0.521735429763794), ('kings', 0.5066401362419128)]\n",
            "2020-02-22 06:26:50,278 : MainThread : INFO : Similarity score between woman and man is 0.6998663 \n",
            "2020-02-22 06:26:50,278 : MainThread : INFO : Finished running --setup\n",
            "2020-02-22 06:26:50,471 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-02-22 06:28:51,246 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
            "2020-02-22 06:28:51,247 : MainThread : INFO : saving Word2VecKeyedVectors object under /root/.cache/nlgeval/glove.6B.300d.model.bin, separately None\n",
            "2020-02-22 06:28:51,247 : MainThread : INFO : storing np array 'vectors' to /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy\n",
            "2020-02-22 06:28:53,031 : MainThread : INFO : not storing attribute vectors_norm\n",
            "2020-02-22 06:28:54,133 : MainThread : INFO : saved /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "2020-02-22 06:28:54,134 : MainThread : INFO : loading Word2VecKeyedVectors object from /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "2020-02-22 06:28:55,565 : MainThread : INFO : loading vectors from /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy with mmap=r\n",
            "2020-02-22 06:28:55,567 : MainThread : INFO : setting ignored attribute vectors_norm to None\n",
            "2020-02-22 06:28:55,568 : MainThread : INFO : loaded /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
            "WARNING: could not read rc.json in /root/.config/nlgeval, overwriting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bo-5slNjEoy",
        "colab_type": "text"
      },
      "source": [
        "Example Documentation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb5ycPDBi535",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "import unittest\n",
        "\n",
        "import nlgeval\n",
        "from nlgeval import NLGEval\n",
        "\n",
        "\n",
        "class TestNlgEval(unittest.TestCase):\n",
        "    def test_compute_metrics_oo(self):\n",
        "        # Create the object in the test so that it can be garbage collected once the test is done.\n",
        "        n = NLGEval()\n",
        "\n",
        "        # Individual Metrics\n",
        "        scores = n.compute_individual_metrics(ref=[\"this is a test\",\n",
        "                                                   \"this is also a test\"],\n",
        "                                              hyp=\"this is a good test\")\n",
        "        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)\n",
        "        self.assertAlmostEqual(0.5108729, scores['Bleu_3'], places=5)\n",
        "        self.assertAlmostEqual(0.0000903602, scores['Bleu_4'], places=5)\n",
        "        self.assertAlmostEqual(0.44434387, scores['METEOR'], places=5)\n",
        "        self.assertAlmostEqual(0.9070631, scores['ROUGE_L'], places=5)\n",
        "        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)\n",
        "        self.assertAlmostEqual(0.8375251, scores['SkipThoughtCS'], places=5)\n",
        "        self.assertAlmostEqual(0.980075, scores['EmbeddingAverageCosineSimilarity'], places=5)\n",
        "        self.assertEqual(scores['EmbeddingAverageCosineSimilarity'], scores['EmbeddingAverageCosineSimilairty'])\n",
        "        self.assertAlmostEqual(0.94509, scores['VectorExtremaCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.960771, scores['GreedyMatchingScore'], places=5)\n",
        "        self.assertEqual(12, len(scores))\n",
        "\n",
        "        scores = n.compute_metrics(ref_list=[\n",
        "            [\n",
        "                \"this is one reference sentence for sentence1\",\n",
        "                \"this is a reference sentence for sentence2 which was generated by your model\"\n",
        "            ],\n",
        "            [\n",
        "                \"this is one more reference sentence for sentence1\",\n",
        "                \"this is the second reference sentence for sentence2\"\n",
        "            ],\n",
        "        ],\n",
        "            hyp_list=[\n",
        "                \"this is the model generated sentence1 which seems good enough\",\n",
        "                \"this is sentence2 which has been generated by your model\"\n",
        "            ]\n",
        "        )\n",
        "        self.assertAlmostEqual(0.55, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(0.428174, scores['Bleu_2'], places=5)\n",
        "        self.assertAlmostEqual(0.284043, scores['Bleu_3'], places=5)\n",
        "        self.assertAlmostEqual(0.201143, scores['Bleu_4'], places=5)\n",
        "        self.assertAlmostEqual(0.295797, scores['METEOR'], places=5)\n",
        "        self.assertAlmostEqual(0.522104, scores['ROUGE_L'], places=5)\n",
        "        self.assertAlmostEqual(1.242192, scores['CIDEr'], places=5)\n",
        "        self.assertAlmostEqual(0.626149, scores['SkipThoughtCS'], places=5)\n",
        "        self.assertAlmostEqual(0.88469, scores['EmbeddingAverageCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.568696, scores['VectorExtremaCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.784205, scores['GreedyMatchingScore'], places=5)\n",
        "        self.assertEqual(12, len(scores))\n",
        "\n",
        "        # Non-ASCII tests.\n",
        "        scores = n.compute_individual_metrics(ref=[\"Test en français.\",\n",
        "                                                   \"Le test en français.\"],\n",
        "                                              hyp=\"Le test est en français.\")\n",
        "        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)\n",
        "        self.assertAlmostEqual(0.0000051, scores['Bleu_3'], places=5)\n",
        "        self.assertAlmostEqual(0, scores['Bleu_4'], places=5)\n",
        "        self.assertAlmostEqual(0.48372379050300296, scores['METEOR'], places=5)\n",
        "        self.assertAlmostEqual(0.9070631, scores['ROUGE_L'], places=5)\n",
        "        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)\n",
        "        self.assertAlmostEqual(0.9192341566085815, scores['SkipThoughtCS'], places=5)\n",
        "        self.assertAlmostEqual(0.906562, scores['EmbeddingAverageCosineSimilarity'], places=5)\n",
        "        self.assertEqual(scores['EmbeddingAverageCosineSimilarity'], scores['EmbeddingAverageCosineSimilairty'])\n",
        "        self.assertAlmostEqual(0.815158, scores['VectorExtremaCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.940959, scores['GreedyMatchingScore'], places=5)\n",
        "        self.assertEqual(12, len(scores))\n",
        "\n",
        "        scores = n.compute_individual_metrics(ref=[\"テスト\"],\n",
        "                                              hyp=\"テスト\")\n",
        "        self.assertAlmostEqual(0.99999999, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(1.0, scores['METEOR'], places=3)\n",
        "        self.assertAlmostEqual(1.0, scores['ROUGE_L'], places=3)\n",
        "        self.assertAlmostEqual(0.0, scores['CIDEr'], places=3)\n",
        "        self.assertAlmostEqual(1.0, scores['SkipThoughtCS'], places=3)\n",
        "        self.assertAlmostEqual(1.0, scores['GreedyMatchingScore'], places=3)\n",
        "        self.assertEqual(12, len(scores))\n",
        "\n",
        "    def test_compute_metrics_omit(self):\n",
        "        n = NLGEval(metrics_to_omit=['Bleu_3', 'METEOR', 'EmbeddingAverageCosineSimilarity'])\n",
        "\n",
        "        # Individual Metrics\n",
        "        scores = n.compute_individual_metrics(ref=[\"this is a test\",\n",
        "                                                   \"this is also a test\"],\n",
        "                                              hyp=\"this is a good test\")\n",
        "        self.assertAlmostEqual(0.799999, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(0.632455, scores['Bleu_2'], places=5)\n",
        "        self.assertAlmostEqual(0.9070631, scores['ROUGE_L'], places=5)\n",
        "        self.assertAlmostEqual(0.0, scores['CIDEr'], places=5)\n",
        "        self.assertAlmostEqual(0.8375251, scores['SkipThoughtCS'], places=5)\n",
        "        self.assertAlmostEqual(0.94509, scores['VectorExtremaCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.960771, scores['GreedyMatchingScore'], places=5)\n",
        "        self.assertEqual(7, len(scores))\n",
        "\n",
        "    def test_compute_metrics(self):\n",
        "        # The example from the README.\n",
        "        root_dir = os.path.join(os.path.dirname(__file__), '..', '..')\n",
        "        hypothesis = os.path.join(root_dir, 'examples/hyp.txt')\n",
        "        references = os.path.join(root_dir, 'examples/ref1.txt'), os.path.join(root_dir, 'examples/ref2.txt')\n",
        "        scores = nlgeval.compute_metrics(hypothesis, references)\n",
        "        self.assertAlmostEqual(0.55, scores['Bleu_1'], places=5)\n",
        "        self.assertAlmostEqual(0.428174, scores['Bleu_2'], places=5)\n",
        "        self.assertAlmostEqual(0.284043, scores['Bleu_3'], places=5)\n",
        "        self.assertAlmostEqual(0.201143, scores['Bleu_4'], places=5)\n",
        "        self.assertAlmostEqual(0.295797, scores['METEOR'], places=5)\n",
        "        self.assertAlmostEqual(0.522104, scores['ROUGE_L'], places=5)\n",
        "        self.assertAlmostEqual(1.242192, scores['CIDEr'], places=5)\n",
        "        self.assertAlmostEqual(0.626149, scores['SkipThoughtCS'], places=5)\n",
        "        self.assertAlmostEqual(0.88469, scores['EmbeddingAverageCosineSimilarity'], places=5)\n",
        "        self.assertEqual(scores['EmbeddingAverageCosineSimilarity'], scores['EmbeddingAverageCosineSimilairty'])\n",
        "        self.assertAlmostEqual(0.568696, scores['VectorExtremaCosineSimilarity'], places=5)\n",
        "        self.assertAlmostEqual(0.784205, scores['GreedyMatchingScore'], places=5)\n",
        "        self.assertEqual(12, len(scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PGGtWUyjPdK",
        "colab_type": "code",
        "outputId": "e0f950a5-7d53-411e-d983-d18fa7061c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "import unittest\n",
        "\n",
        "import nlgeval\n",
        "from nlgeval import NLGEval\n",
        "\n",
        "\n",
        "n = NLGEval()\n",
        "\n",
        "# Individual Metrics\n",
        "scores = n.compute_individual_metrics(ref=[\"this is a test\",\"this is also a test\",\"this is test\",\"this is really a test\",\"yes, this is a test\"],hyp=\"this is a good test\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcnqIfxejba5",
        "colab_type": "code",
        "outputId": "cbb94eca-8e49-49bd-8f54-444cb886df21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# print(scores)\n",
        "print(\"Bleu_1 : \",scores['Bleu_1'])\n",
        "print(\"Bleu_2 : \",scores['Bleu_2'])\n",
        "print(\"Bleu_3 : \",scores['Bleu_3'])\n",
        "print(\"Bleu_4 : \",scores['Bleu_4'])\n",
        "print(\"METEOR : \",scores['METEOR'])\n",
        "print(\"ROUGE_L : \",scores['ROUGE_L'])\n",
        "print(\"CIDEr : \",scores['CIDEr'])\n",
        "print(\"SkipThoughtCS : \",scores['SkipThoughtCS'])\n",
        "print(\"EmbeddingAverageCosineSimilarity : \",scores['EmbeddingAverageCosineSimilarity'])\n",
        "print(\"VectorExtremaCosineSimilarity : \",scores['VectorExtremaCosineSimilarity'])\n",
        "print(\"GreedyMatchingScore : \",scores['GreedyMatchingScore'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu_1 :  0.7999999996800004\n",
            "Bleu_2 :  0.6324555317648827\n",
            "Bleu_3 :  0.5108729546934666\n",
            "Bleu_4 :  9.036020031392194e-05\n",
            "METEOR :  0.4443438716486749\n",
            "ROUGE_L :  0.9070631970260222\n",
            "CIDEr :  0.0\n",
            "SkipThoughtCS :  0.837525\n",
            "EmbeddingAverageCosineSimilarity :  0.983229\n",
            "VectorExtremaCosineSimilarity :  0.94509\n",
            "GreedyMatchingScore :  0.960771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezi5ZQUsjdag",
        "colab_type": "text"
      },
      "source": [
        "## Using \"pycocoevalcap\" Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rciz55us6zP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "076996c9-64b0-4923-cea7-81d495c7e259"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JF9YpeQtAcl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d07482a9-4fcf-415b-bafd-747e92273cd9"
      },
      "source": [
        "pip install pycocotools"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3zCHWxFjlPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "769b401a-1e0a-48f6-83da-27e8bc43c1f3"
      },
      "source": [
        "!pip install git+https://github.com/salaniz/pycocoevalcap"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-wlrxqwr8\n",
            "  Running command git clone -q https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-wlrxqwr8\n",
            "Requirement already satisfied (use --upgrade to upgrade): pycocoevalcap==1.1 from git+https://github.com/salaniz/pycocoevalcap in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: pycocotools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from pycocoevalcap==1.1) (2.0.0)\n",
            "Building wheels for collected packages: pycocoevalcap\n",
            "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.1-py3-none-any.whl size=104310172 sha256=6b58ce6a9048212c7658eb3b8c92401cd89d44b697334fe87018d26fb79d5811\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tr57ri1u/wheels/df/74/69/758b2491ca93bf681a1509671df34df9cf5ff605edf6e112ed\n",
            "Successfully built pycocoevalcap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5KJJmyVju-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEo5BKa30smo",
        "colab_type": "text"
      },
      "source": [
        "Evaluating captions from JSON files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O20pGrs3uRnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "ff2134a6-5253-4e11-daad-1dc410b2a4a7"
      },
      "source": [
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "import json\n",
        "\n",
        "with open('/content/gts.json', 'r') as file:\n",
        "    gts = json.load(file)\n",
        "with open('/content/res.json', 'r') as file:\n",
        "    res = json.load(file)\n",
        "\n",
        "def bleu():\n",
        "    scorer = Bleu(n=4)\n",
        "    # scorer += (hypo[0], ref1)   # hypo[0] = 'word1 word2 word3 ...'\n",
        "    #                                 # ref = ['word1 word2 word3 ...', 'word1 word2 word3 ...']\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "\n",
        "    print('belu = %s' % score)\n",
        "\n",
        "def cider():\n",
        "    scorer = Cider()\n",
        "    # scorer += (hypo[0], ref1)\n",
        "    (score, scores) = scorer.compute_score(gts, res)\n",
        "    print('cider = %s' % score)\n",
        "\n",
        "def meteor():\n",
        "    scorer = Meteor()\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "    print('meter = %s' % score)\n",
        "\n",
        "def rouge():\n",
        "    scorer = Rouge()\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "    print('rouge = %s' % score)\n",
        "\n",
        "def spice():\n",
        "    scorer = Spice()\n",
        "    score, scores = scorer.compute_score(gts, res)\n",
        "    print('spice = %s' % score)\n",
        "\n",
        "def main():\n",
        "    bleu()\n",
        "    cider()\n",
        "    meteor()\n",
        "    rouge()\n",
        "    spice()\n",
        "main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'testlen': 9893, 'reflen': 9855, 'guess': [9893, 8893, 7893, 6893], 'correct': [5732, 2510, 1043, 423]}\n",
            "ratio: 1.003855910705124\n",
            "belu = [0.5793995754573356, 0.40439129741018104, 0.2785363856619147, 0.1908290437674253]\n",
            "cider = 0.5997905185184199\n",
            "meter = 0.19525467177780284\n",
            "rouge = 0.39625269357570847\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "spice = 0.13288891803208092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_94s_xY2jyVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class COCOEvalCap:\n",
        "    def __init__(self, coco, cocoRes):\n",
        "        self.evalImgs = []\n",
        "        self.eval = {}\n",
        "        self.imgToEval = {}\n",
        "        self.coco = coco\n",
        "        self.cocoRes = cocoRes\n",
        "        self.params = {'image_id': coco.getImgIds()}\n",
        "\n",
        "    def evaluate(self):\n",
        "        imgIds = self.params['image_id']\n",
        "        # imgIds = self.coco.getImgIds()\n",
        "        gts = {}\n",
        "        res = {}\n",
        "        for imgId in imgIds:\n",
        "            gts[imgId] = self.coco.imgToAnns[imgId]\n",
        "            res[imgId] = self.cocoRes.imgToAnns[imgId]\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenization...')\n",
        "        tokenizer = PTBTokenizer()\n",
        "        gts  = tokenizer.tokenize(gts)\n",
        "        res = tokenizer.tokenize(res)\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "            (Meteor(),\"METEOR\"),\n",
        "            (Rouge(), \"ROUGE_L\"),\n",
        "            (Cider(), \"CIDEr\"),\n",
        "            (Spice(), \"SPICE\")\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(gts, res)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, gts.keys(), m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, gts.keys(), method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = {}\n",
        "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeK-Fml8j1eE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_scores(ref, hypo):\n",
        "    \"\"\"\n",
        "    ref, dictionary of reference sentences (id, sentence)  ------> here, each element of ref is a list\n",
        "    hypo, dictionary of hypothesis sentences (id, sentence)\n",
        "    score, dictionary of scores\n",
        "    \"\"\"\n",
        "    print(type(ref))\n",
        "    scorers = [\n",
        "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "        (Meteor(),\"METEOR\"),\n",
        "        (Rouge(), \"ROUGE_L\"),\n",
        "        (Cider(), \"CIDEr\"),\n",
        "        (Spice(), \"SPICE\")\n",
        "    ]\n",
        "    final_scores = {}\n",
        "    for scorer, method in scorers:\n",
        "        score, scores = scorer.compute_score(ref, hypo)\n",
        "        if type(score) == list:\n",
        "            for m, s in zip(method, score):\n",
        "                final_scores[m] = s\n",
        "        else:\n",
        "            final_scores[method] = score\n",
        "    return final_scores "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1vsIWPyj4Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref = {}\n",
        "hypo = {}\n",
        "\n",
        "ref[0] = [\"this is one reference sentence for sentence1\",\"this is a reference sentence for sentence2 which was generated by your model\"]\n",
        "hypo[0] = [\"this is the model generated sentence1 which seems good enough\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcoMtH7cj7yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "references = [[\"this is one reference sentence for sentence1\",\"this is a reference sentence for sentence2 which was generated by your model\"],[\"this is one more reference sentence for sentence1\",\"this is the second reference sentence for sentence2\"],],\n",
        "hypothesis = [\"this is the model generated sentence1 which seems good enough\",\"this is sentence2 which has been generated by your model\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHBbL8Mmj8dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5e391f06-36dc-401b-a1a3-a12f94885f1c"
      },
      "source": [
        "calc_scores(ref, hypo)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'testlen': 10, 'reflen': 7, 'guess': [10, 9, 8, 7], 'correct': [6, 1, 0, 0]}\n",
            "ratio: 1.4285714283673472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bleu_1': 0.5999999999400001,\n",
              " 'Bleu_2': 0.25819888971990695,\n",
              " 'Bleu_3': 2.027400664963992e-06,\n",
              " 'Bleu_4': 5.873949093995857e-09,\n",
              " 'CIDEr': 0.0,\n",
              " 'METEOR': 0.16741198894585999,\n",
              " 'ROUGE_L': 0.36454183266932266,\n",
              " 'SPICE': 0.36363636363636365}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZosIAGg6kP3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37e02081-2448-46da-f0a5-678f9482b6f9"
      },
      "source": [
        "from pycocotools import mask as mask\n",
        "print(mask.__author__)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tsungyi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMiVwwp4kTYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_scores(ref, hypo):\n",
        "    \"\"\"\n",
        "    ref, dictionary of reference sentences (id, sentence)\n",
        "    hypo, dictionary of hypothesis sentences (id, sentence)\n",
        "    score, dictionary of scores\n",
        "    \"\"\"\n",
        "    print(type(hypo.keys))\n",
        "    scorers = [\n",
        "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "        (Meteor(),\"METEOR\"),\n",
        "        (Rouge(), \"ROUGE_L\"),\n",
        "        (Cider(), \"CIDEr\"),\n",
        "        (Spice(), \"SPICE\")\n",
        "    ]\n",
        "    final_scores = {}\n",
        "    for scorer, method in scorers:\n",
        "        score, scores = scorer.compute_score(ref, hypo)\n",
        "        if type(score) == list:\n",
        "            for m, s in zip(method, score):\n",
        "                final_scores[m] = s\n",
        "        else:\n",
        "            final_scores[method] = score\n",
        "    return final_scores "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r775JBbQkk_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref = {}\n",
        "hypo = {}\n",
        "\n",
        "ref[0] = [\"this is one reference sentence for sentence1\",\"this is a reference sentence for sentence2 which was generated by your model\"]\n",
        "ref[1] = [\"this is one more reference sentence for sentence1\",\"this is the second reference sentence for sentence2\"]\n",
        "hypo[0] = [\"this is the model generated sentence1 which seems good enough\"]\n",
        "hypo[1] = [\"this is sentence2 which has been generated by your model\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWLEmhUbkohJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d1bdceed-15a2-4248-b395-593964e09273"
      },
      "source": [
        "print(calc_scores(ref, hypo))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'builtin_function_or_method'>\n",
            "{'testlen': 20, 'reflen': 15, 'guess': [20, 18, 16, 14], 'correct': [9, 2, 0, 0]}\n",
            "ratio: 1.3333333332444444\n",
            "{'Bleu_1': 0.44999999997750006, 'Bleu_2': 0.22360679773817757, 'Bleu_3': 1.4620088690245348e-06, 'Bleu_4': 3.8652758782383096e-09, 'METEOR': 0.1552910794167853, 'ROUGE_L': 0.3523452657770405, 'CIDEr': 0.23638309890658804, 'SPICE': 0.2727272727272727}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QThUjCpikuo1",
        "colab_type": "text"
      },
      "source": [
        "## Using nltk Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcd0oOfkky80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate import AlignedSent, Alignment\n",
        "import nltk.translate.gleu_score\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from nltk.translate.gleu_score import corpus_gleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f53DL398mloQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which','ensures', 'that', 'the', 'military', 'always','obeys', 'the', 'commands', 'of', 'the', 'party']\n",
        "ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that','ensures', 'that', 'the', 'military', 'will', 'forever','heed', 'Party', 'commands']\n",
        "ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',          'guarantees', 'the', 'military', 'forces', 'always','being', 'under', 'the', 'command', 'of', 'the', 'Party']\n",
        "ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the','army', 'always', 'to', 'heed', 'the', 'directions','of', 'the', 'party']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5bkjuG9mq0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was','interested', 'in', 'world', 'history']\n",
        "ref2a = ['he', 'was', 'interested', 'in', 'world', 'history','because', 'he', 'read', 'the', 'book']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xOGMNSVmtyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which','ensures', 'that', 'the', 'military', 'always','obeys', 'the', 'commands', 'of', 'the', 'party']\n",
        "hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops','forever', 'hearing', 'the', 'activity', 'guidebook','that', 'party', 'direct']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCxEQijimxHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that','ensures', 'that', 'the', 'military', 'will', 'forever','heed', 'Party', 'commands']\n",
        " reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which','guarantees', 'the', 'military', 'forces', 'always','being', 'under', 'the', 'command', 'of', 'the','Party']\n",
        " reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the','army', 'always', 'to', 'heed', 'the', 'directions','of', 'the', 'party']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrFejjTdm2v_",
        "colab_type": "code",
        "outputId": "170fd507-7d92-4883-f619-0c40429ba1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n",
        "hypotheses = [hyp1, hyp2]\n",
        "corpus_gleu(list_of_references, hypotheses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5673076923076923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DocdN93nBJ4",
        "colab_type": "code",
        "outputId": "19379a73-0715-4e2b-d35b-2794628a8df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus_bleu(list_of_references, hypotheses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5920778868801042"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfJ23M13nDSX",
        "colab_type": "code",
        "outputId": "bfb4000c-693f-45f8-d9bb-93a065c9fb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)\n",
        "score2 = sentence_bleu([ref2a], hyp2)\n",
        "(score1 + score2) / 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6223247442490669"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTMlWNcunF94",
        "colab_type": "code",
        "outputId": "9d18431b-1d32-4088-becb-c5361f21f8bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence_bleu([reference1, reference2, reference3], hypothesis1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5045666840058485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jijq-ljKnxQP",
        "colab_type": "code",
        "outputId": "a52a595f-2220-44e1-cc6f-53b3be12edbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence_gleu([reference1, reference2, reference3], hypothesis1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4393939393939394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llsznf52nJh4",
        "colab_type": "code",
        "outputId": "d91ba7ed-618d-44f4-ed42-4f58bfca27d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3969"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Ttss2dnLoY",
        "colab_type": "code",
        "outputId": "8a7a19da-6ace-4265-837a-87c98bcc4ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chencherry = SmoothingFunction()\n",
        "sentence_bleu([reference1, reference2, reference3], hypothesis2,smoothing_function=chencherry.method1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03703131191121491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP_Rf4QMnPIY",
        "colab_type": "code",
        "outputId": "e9ea785d-0566-4e6e-b943-c5efeef0c81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weights = (.25, .25, .25, .25)\n",
        "sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5045666840058485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q82HHgG0n2R3",
        "colab_type": "text"
      },
      "source": [
        "More Information can be obtained from <a href=\"https://www.nltk.org/api/nltk.translate.html\">this link</a> ."
      ]
    }
  ]
}